{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for News Website Articles Topic Modelling\n",
    "\n",
    "The scrapper contained in the folder will crawl the web by parsing a csv file with urls. The scrapper will provide us with a csv file containing the url,website title and description for each url in the original csv. \n",
    "For this project, we obtained a list of urls to articles' websites. This notebook performs NLP on the output dataset from the scrapper, and consists of:\n",
    "\n",
    "1) Topic Modelling with LDA:\n",
    "\n",
    "- Aimed at constructing a good dictionary by extracting all possible vocabularies/words from the dataset and remove those that are not relevant/informative (e.g., the infrequent words). \n",
    "\n",
    "- For url, given \"www.lachainemeteo.com/meteo-france/ville-6560/previsions-meteo-bordeaux-aujourdhui\" the following words can be extracted: lachainemeteo (or www.lachainemeteo.com), meteo-france, ville, prevision, meteo, bordeaux.\n",
    "\n",
    "- Built a first/baseline model/method using LDA (Latent Dirichlet Allocation from the Gensim library) to find the topics from the constructed dictionary. An important aspect is properly defining the number of topics for LDA.\n",
    "\n",
    "- Applied LDA to both titles and descriptions of the websites seperately. Finally, we combine all columns containing description, title and url to construct the dictionary and apply LDA. We then minimize topic coherence metrics vs. Topics number in LDA to find the best number of topics\n",
    "\n",
    "2) Improving Baseline model:\n",
    "\n",
    "To find the best number of topics, we have used two methods:\n",
    "    - Calculated topic coherence for several number of topics to obtain optimal number of topics\n",
    "    - Clustered embedding with K-means and used the elbow method to find optimal number of clusters\n",
    "\n",
    "- For titles, used LSA to obtain SVD_matrix, performed k-means clustering on vectors representations, and applied LDA on each cluster to define 1 topic per cluster \n",
    "\n",
    "- For descriptions, implemented word2vec, performed k-means clustering on vectors representations, applied TF/IDF and other preprocessing techniques to keep only useful words (specially for descriptions), and applied LDA on each cluster to define 1 topic per cluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing URL data and creating data2.csv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"scrapped_url.csv\", delimiter=\",\")\n",
    "df = df.dropna(subset=['title', 'description']).drop(columns=\"Unnamed: 3\")\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    # taking care of www., .com, http and https.. and tokenizing\n",
    "    if df.iloc[i,0][4]==\"s\":\n",
    "        df.iloc[i,0] = str(df.iloc[i,0][8:]).replace(\"www.\",\"\").replace(\".comment\",\". comment\").replace(\".madame\",\". madame\").replace(\".com\",\"\").replace(\".php\",\"\").replace(\".fr\",\"\").replace(\".html\",\"\").replace(\".\",\" \").replace(\"-\",\" \").replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").replace(\"3\",\"\").replace(\"4\",\"\").replace(\"5\",\"\").replace(\"6\",\"\").replace(\"7\",\"\").replace(\"8\",\"\").replace(\"9\",\"\").strip().split(\"/\")\n",
    "        df.iloc[i,0] = str(\" \".join(df.iloc[i,0]).strip().replace(\"     \",\" \").replace(\"  \",\" \").lower().replace(\" artfig\",\"\")).split(\" \")\n",
    "    else:\n",
    "        df.iloc[i,0] = str(df.iloc[i,0][7:]).replace(\"www.\",\"\").replace(\".comment\",\". comment\").replace(\".madame\",\". madame\").replace(\".com\",\"\").replace(\".php\",\"\").replace(\".fr\",\"\").replace(\".html\",\"\").replace(\".\",\" \").replace(\"-\",\" \").replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").replace(\"3\",\"\").replace(\"4\",\"\").replace(\"5\",\"\").replace(\"6\",\"\").replace(\"7\",\"\").replace(\"8\",\"\").replace(\"9\",\"\").strip().split(\"/\")\n",
    "        df.iloc[i,0] = str(\" \".join(df.iloc[i,0]).strip().replace(\"     \",\" \").replace(\"  \",\" \").lower().replace(\" artfig\",\"\")).split(\" \")\n",
    "\n",
    "df.to_csv(\"data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: Analysing URLs with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['url_clean'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Analysing Titles with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['title'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: Analysing Descriptions with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['description'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model: LDA on combined URL, TITLES and DESCRIPTIONS tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_url = df['title','url_clean','description'].map(preprocess)\n",
    "processed_url\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_url)\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_url]\n",
    "\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')\n",
    "\n",
    "NUM_TOPICS = 9\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model: Coherence values analysis for best number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pyLDAvis.gensim\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "\n",
    "# Import dataset\n",
    "p_df = pd.read_csv('data2.csv')\n",
    "p_df['all'] = p_df[['description', 'title', 'url_clean']].apply(lambda x: ' '.join(x).replace('[','').replace(']','').replace(\"'\",\"\").replace(\".\",\"\").replace(\",\",\"\").replace(\";\",\"\"), axis=1)\n",
    "\n",
    "docs =array(p_df['all'])\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "# Define function for tokenize and lemmatizing\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove unallowed_words and with less than 4 characters\n",
    "    docs = [[token for token in doc if len(token) > 4 and token not in unallowed_words] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs\n",
    "# Perform function on our document\n",
    "docs = docs_preprocessor(docs)\n",
    "#Create Biagram & Trigram Models \n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 2 times or more.\n",
    "bigram = Phrases(docs, min_count=2)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "#Remove rare & common tokens \n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.2)\n",
    "#Create dictionary and corpus required for Topic Modeling\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    coherence_values2 = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherencemodel2 = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        coherence_values2.append(coherencemodel2.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values, coherence_values2\n",
    "model_list, coherence_values, coherence_values2 = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=2, limit=40, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=39; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(x, coherence_values,label=\"c_v\")\n",
    "plt.title('Topic Coherence Values')\n",
    "plt.ylabel('c_v')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(x, coherence_values2,label=\"u_mass\")\n",
    "plt.xlabel('Num Topics')\n",
    "plt.ylabel('u_mass')\n",
    "\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters.\n",
    "num_topics = 8\n",
    "chunksize = 500 \n",
    "passes = 20 \n",
    "iterations = 400\n",
    "eval_every = 1  \n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)\n",
    "\n",
    "# Print the Keyword in the topics\n",
    "print(lda_model.print_topics())\n",
    "\n",
    "\"\"\"\n",
    "# Compute Coherence Score using c_v and UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_model_lda_umass = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence=\"u_mass\")\n",
    "coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "coherence_lda_umass = coherence_model_lda_umass.get_coherence()\n",
    "print('\\nCoherence Score_c_v: ', coherence_lda_cv)\n",
    "print('\\nCoherence Score_umass: ', coherence_lda_umass)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model: Analysing titles with LSA and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSA: https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "documents = df[\"title\"].values\n",
    "unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "\n",
    "# some preprocessing\n",
    "for r in range(0,len(documents)):\n",
    "    documents[r] = ''.join(i for i in documents[r] if not i.isdigit())\n",
    "    \n",
    "    # Removing words with less than XX characters\n",
    "    XX = 4\n",
    "    documents[r] = ' '.join([w for w in documents[r].split() if len(w)>XX and w not in unallowed_words])\n",
    "\n",
    "# raw documents to tf-idf matrix: \n",
    "vectorizer = TfidfVectorizer(use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "# SVD to reduce dimensionality: \n",
    "svd_model = TruncatedSVD(n_components=1000,\n",
    "                         algorithm='randomized',\n",
    "                         n_iter=100)\n",
    "# pipeline of tf-idf + SVD, fit to and applied to documents:\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(documents)\n",
    "# svd_matrix can later be used to compare documents, compare words, or compare queries with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimum number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NBR_CLUSTER_TEST = 20\n",
    "x = []\n",
    "for j in range(0,len(svd_matrix)):\n",
    "    x.append(svd_matrix[j])\n",
    "wcss = [] #Within Cluster Sum of Squares\n",
    "\n",
    "for i in range(1, NBR_CLUSTER_TEST):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, NBR_CLUSTER_TEST), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying kmeans to the dataset / Creating the kmeans classifier\n",
    "BEST_CLUSTER_NBR = 7\n",
    "kmeans = KMeans(n_clusters = BEST_CLUSTER_NBR, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "y_kmeans = kmeans.fit_predict(x)\n",
    "df[\"kmeans_class\"] = y_kmeans\n",
    "df.groupby('kmeans_class').count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LDA to find 1 topic per cluster\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    unallowed_words = [\"lefigaro\",\"figaro\",\"linternaute\",\"commentcamarche\",\"journaldunet\",\"lachainemeteo\",\"linternaute\",\"internaute\",\"madame\"]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 4 and token not in unallowed_words:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "CLUSTER_NUM = 7\n",
    "for i in range(0,CLUSTER_NUM):\n",
    "    # Latent DIrichlet Allocation Model\n",
    "    processed_desc = df.loc[df['kmeans_class']==i][\"title\"].map(preprocess)\n",
    "    dictionary = corpora.Dictionary(processed_desc)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_desc]\n",
    "\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 1, id2word=dictionary, passes=15)\n",
    "    #ldamodel.save(f\"model{NUM_TOPICS}.gensim\")\n",
    "    for idx, topic in ldamodel.print_topics(-1):\n",
    "        print('Cluster: {} \\n Topic: {} \\nWords: {}'.format(i, ldamodel.print_topics(num_words=1), topic))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model: Analysing Descriptions with Word2vec and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model_w2v_path = './model_w2v_urls.bin'\n",
    "df2 = pd.read_csv(\"data2.csv\")\n",
    "X_train = df2[\"description\"]\n",
    "emb_size = 128\n",
    "\n",
    "# some preprocessing\n",
    "XX = 4\n",
    "for r in range(0,len(X_train)):\n",
    "    X_train[r] = ''.join(i for i in X_train[r] if not i.isdigit())\n",
    "    X_train[r] = ' '.join([w for w in X_train[r].split() if len(w)>XX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building/training the model\n",
    "# Initialize model_w2v and build vocabularies\n",
    "# should use the whole dataset, not just training set\n",
    "model_w2v = Word2Vec(size=emb_size, min_count=5)\n",
    "model_w2v.build_vocab(X_train)\n",
    "model_w2v.train(X_train, total_examples=model_w2v.corpus_count, epochs=2000)\n",
    "\n",
    "# save the model_w2v\n",
    "model_w2v.save(model_w2v_path)\n",
    "print(\"training w2v: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the trained model_w2v, buidling vectors and cleaning dataframe\n",
    "new_model = Word2Vec.load(model_w2v_path)\n",
    "X = new_model[new_model.wv.vocab] # get words\n",
    "\n",
    "def build_word2vec_from_text(model_w2v, sentence, emb_size):\n",
    "    emb_vec = np.zeros(emb_size).reshape((1, emb_size))\n",
    "    count = 0.\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            emb_vec += model_w2v[word].reshape((1, emb_size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        emb_vec /= count\n",
    "    return emb_vec\n",
    "\n",
    "def scalar_2vec(data_frame, column_names):\n",
    "    new_frame = data_frame.loc[:, column_names]\n",
    "    return new_frame\n",
    "\n",
    "X_train = np.concatenate([build_word2vec_from_text(new_model, d, emb_size) for d in X_train]) #model_w2v\n",
    "newdf = pd.concat([df2, pd.DataFrame(X_train)],  axis=1)\n",
    "newdf[\"vect\"] = newdf[list(range(0,128))].values.tolist()\n",
    "selected_columns = [\"url_clean\",\"title\",\"description\",\"vect\"]\n",
    "newdf = scalar_2vec(newdf, selected_columns)\n",
    "newdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing vectors\n",
    "from sklearn import preprocessing\n",
    "\n",
    "xvect = []\n",
    "for i in range(0,len(newdf[\"vect\"])):\n",
    "    xvect.append(np.array(newdf[\"vect\"].values[0]))\n",
    "\n",
    "\"\"\"\n",
    "## tried to standardize, normalize and scale vectors, but no improvements\n",
    "standardized_vect = preprocessing.scale(xvect)\n",
    "normalized_vect = preprocessing.normalize(xvect, norm='l2')\n",
    "scaled_vect = preprocessing.scale(xvect)\n",
    "newdf[\"standard_vect\"] = list(standardized_vect)\n",
    "newdf[\"normal_vect\"] = list(normalized_vect)\n",
    "newdf[\"scaled_vect\"] = list(scaled_vect)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding optimum number of clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NBR_CLUSTER_TEST = 14\n",
    "x = []\n",
    "for j in range(0,len(newdf)):\n",
    "    x.append(newdf[\"vect\"][j])\n",
    "wcss = [] #Within Cluster Sum of Squares\n",
    "\n",
    "for i in range(1, NBR_CLUSTER_TEST):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(x)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, NBR_CLUSTER_TEST), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying kmeans to the dataset / Creating the kmeans classifier\n",
    "BEST_CLUSTER_NBR = 10\n",
    "kmeans = KMeans(n_clusters = BEST_CLUSTER_NBR, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "y_kmeans = kmeans.fit_predict(x)\n",
    "newdf[\"kmeans_class\"] = y_kmeans\n",
    "newdf.groupby('kmeans_class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Analysis\n",
    "#We want to plot the number of words of description text for each cluster\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "CLUSTER_NUM = 8\n",
    "for i in range(0,CLUSTER_NUM):\n",
    "    #TfidfVectorizer: Converts a collection of raw documents to a matrix of TF-IDF features.\n",
    "    #min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "    #max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "    #Apply this vectorizer to the full dataset to create normalized vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df = 0.95, sublinear_tf=True, use_idf=True)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(newdf[\"description\"].values)\n",
    "\n",
    "    #tfidf_vectorizer.get_feature_names(): Array mapping from feature integer indices to feature name\n",
    "    features = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    #Get the row that belongs to cluster 0\n",
    "    row = newdf.loc[newdf['kmeans_class']==i].index.tolist()\n",
    "    \n",
    "    \"\"\"\n",
    "    #Create a series from the sparse matrix\n",
    "    word2_matrix = pd.Series(tfidf_matrix.getrow(row).toarray().flatten(),index = features).sort_values(ascending=False)\n",
    "    \"\"\"\n",
    "    \n",
    "    word2_matrix = pd.Series()\n",
    "    for rows in row:\n",
    "        word2_matrix = pd.concat([word2_matrix, pd.Series(tfidf_matrix.getrow(rows).toarray().flatten(),index = features).sort_values(ascending=False)])\n",
    "    word2_matrix = word2_matrix.sort_index(axis=0)\n",
    "    \n",
    "    tf_idf_plot = word2_matrix[:30].plot(kind='bar', title='Article Word TF-IDF Values',\n",
    "                figsize=(10,6), alpha=1, fontsize=14, rot=80,edgecolor='black', linewidth=2 )\n",
    "    tf_idf_plot.title.set_size(18)\n",
    "    tf_idf_plot.set_xlabel('WORDS')\n",
    "    tf_idf_plot.set_ylabel('TF-IDF')\n",
    "    \n",
    "    # Latent DIrichlet Allocation Model\n",
    "    processed_desc = word2_matrix[0:1000].index.map(preprocess)\n",
    "    dictionary = corpora.Dictionary(processed_desc)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_desc]\n",
    "\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "\n",
    "    try:\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 1, id2word=dictionary, passes=15)\n",
    "        #ldamodel.save(f\"model{NUM_TOPICS}.gensim\")\n",
    "        for idx, topic in ldamodel.print_topics(-1):\n",
    "            print('dataframe: {} \\n topic: {} \\nWords: {}'.format(i, idx, topic))\n",
    "    except:\n",
    "        print(f\"LDA FAILED FOR CLUSTER: {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
