{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_clean</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cuisine.journaldesfemmes.fr/recette-de...</td>\n",
       "      <td>Recettes de desserts faciles et rapides</td>\n",
       "      <td>Les meilleures recettes desserts; classiques o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://droit-finances.commentcamarche.com/faq...</td>\n",
       "      <td>Salaire d'une assistante maternelle : ce qu'il...</td>\n",
       "      <td>La rémunération des assistantes maternelles (s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://premium.lefigaro.fr/</td>\n",
       "      <td>Le Figaro Premium - Actualité</td>\n",
       "      <td>Accédez à l’intégralité des articles du Figaro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exclusives réservées aux abonnés pour tout sav...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.commentcamarche.net/faq/8887-voir-...</td>\n",
       "      <td>Voir ses factures Free - Mon compte</td>\n",
       "      <td>Voir le tutoriel en vidéo : Télécharger l'appl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url_clean  \\\n",
       "0  https://cuisine.journaldesfemmes.fr/recette-de...   \n",
       "1  https://droit-finances.commentcamarche.com/faq...   \n",
       "2                        http://premium.lefigaro.fr/   \n",
       "3  exclusives réservées aux abonnés pour tout sav...   \n",
       "4  https://www.commentcamarche.net/faq/8887-voir-...   \n",
       "\n",
       "                                               title  \\\n",
       "0            Recettes de desserts faciles et rapides   \n",
       "1  Salaire d'une assistante maternelle : ce qu'il...   \n",
       "2                      Le Figaro Premium - Actualité   \n",
       "3                                                NaN   \n",
       "4                Voir ses factures Free - Mon compte   \n",
       "\n",
       "                                         description  \n",
       "0  Les meilleures recettes desserts; classiques o...  \n",
       "1  La rémunération des assistantes maternelles (s...  \n",
       "2  Accédez à l’intégralité des articles du Figaro...  \n",
       "3                                                NaN  \n",
       "4  Voir le tutoriel en vidéo : Télécharger l'appl...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "df = pd.read_csv(\"scrapped_test.csv\", delimiter=\",\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing URLs with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(df)):\n",
    "    df.iloc[i,0] = str(df.iloc[i,0][8:]).replace(\"www.\",\"\").replace(\".comment\",\". comment\").replace(\".com\",\"\").replace(\".php\",\"\").replace(\".fr\",\"\").replace(\".html\",\"\").replace(\".\",\" \").replace(\"-\",\" \").replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").replace(\"3\",\"\").replace(\"4\",\"\").replace(\"5\",\"\").replace(\"6\",\"\").replace(\"7\",\"\").replace(\"8\",\"\").replace(\"9\",\"\").strip().split(\"/\")\n",
    "    df.iloc[i,0] = str(\" \".join(df.iloc[i,0]).strip().replace(\"     \",\" \").replace(\"  \",\" \").lower().replace(\" artfig\",\"\")).split(\" \")\n",
    "df.to_csv(\"data2.csv\")\n",
    "df = pd.read_csv(\"data2.csv\")\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "processed_docs = df['url_clean'].map(preprocess)\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent DIrichlet Allocation Model\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_docs]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "import gensim\n",
    "NUM_TOPICS = 10\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save(f\"model{NUM_TOPICS}.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words=1)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing titles and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can first do tf-idf to remove unnecessary words and add to list\n",
    "# then perform LDA again to find topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"La rémunération des assistantes maternelles (salaire horaire; indemnités d'entretien et de nourriture...) doit respecter certains montants minimum. Voici un point sur les droits de l'assistante maternelle et les obligations des parents en... \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Salaire d'une assistante maternelle : ce qu'il faut savoir\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.91629073, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.3996885 , 1.55141507, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.92293899, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 1.97269842, 1.92293899, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.3996885 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.73487781, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenizing data with tf-idf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_sentences = df[\"description\"].dropna()\n",
    "\n",
    "vocab_size = 15000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_matrix(train_sentences, mode='tfidf')\n",
    "#x_test = tokenizer.texts_to_matrix(test_sentences, mode='tfidf')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  ['Les', 'meilleures', 'recettes', 'desserts', ';', 'classiques', 'ou', 'originaux', ';', 'à', 'réaliser', 'rapidement', 'et', 'facilement', 'à', 'la', 'maison', '.']\n",
      "after removing stop words:  ['Les', 'meilleures', 'recettes', 'desserts', ';', 'classiques', 'originaux', ';', 'réaliser', 'rapidement', 'facilement', 'maison', '.']\n",
      "before:  ['La', 'rémunération', 'des', 'assistantes', 'maternelles', '(', 'salaire', 'horaire', ';', 'indemnités', \"d'entretien\", 'et', 'de', 'nourriture', '...', ')', 'doit', 'respecter', 'certains', 'montants', 'minimum', '.', 'Voici', 'un', 'point', 'sur', 'les', 'droits', 'de', \"l'assistante\", 'maternelle', 'et', 'les', 'obligations', 'des', 'parents', 'en', '...']\n",
      "after removing stop words:  ['La', 'rémunération', 'assistantes', 'maternelles', '(', 'salaire', 'horaire', ';', 'indemnités', \"d'entretien\", 'nourriture', '...', ')', 'doit', 'respecter', 'certains', 'montants', 'minimum', '.', 'Voici', 'point', 'les', 'droits', \"l'assistante\", 'maternelle', 'les', 'obligations', 'parents', '...']\n",
      "before:  ['Accédez', 'à', 'l', '’', 'intégralité', 'des', 'articles', 'du', 'Figaro', 'en', 'illimité', '.', 'Politique', ';', 'économie', ';', 'culture', ';', 'international…', 'Retrouvez', 'les', 'analyses', ';', 'les', 'investigations', 'et', 'les', 'enquêtes']\n",
      "after removing stop words:  ['Accédez', '’', 'intégralité', 'articles', 'Figaro', 'illimité', '.', 'Politique', ';', 'économie', ';', 'culture', ';', 'international…', 'Retrouvez', 'les', 'analyses', ';', 'les', 'investigations', 'les', 'enquêtes']\n",
      "before:  ['Voir', 'le', 'tutoriel', 'en', 'vidéo', ':', 'Télécharger', \"l'application\", 'Free', 'Mon', 'compte', 'pour', 'iPhone', ';', 'puis', 'saisir', 'son', 'numéro', 'de', 'téléphone', 'et', 'son', 'mot', 'de', 'passe', 'sur', \"l'écran\", \"d'accueil\", '.', 'Télécharger', \"l'application\", 'officiel', 'Free', 'Mon', 'compte', 'pour', 'Android', '...']\n",
      "after removing stop words:  ['Voir', 'tutoriel', 'vidéo', ':', 'Télécharger', \"l'application\", 'Free', 'Mon', 'compte', 'iPhone', ';', 'puis', 'saisir', 'numéro', 'téléphone', 'mot', 'passe', \"l'écran\", \"d'accueil\", '.', 'Télécharger', \"l'application\", 'officiel', 'Free', 'Mon', 'compte', 'Android', '...']\n",
      "before:  ['Un', 'avenant', 'au', 'contrat', 'de', 'travail', 'peut', 'parfois', 'être', 'refusé', 'par', 'le', 'salarié', '.', 'Voici', 'ses', 'droits', 'et', 'ses', 'obligations', 'lorsque', \"l'employeur\", 'lui', 'demande', 'de', 'signer', 'ce', 'type', 'de', 'document', '.', 'Avant', 'toute', 'chose', ';', 'le', 'salarié', 'doit', 'savoir', 'que', 'le', 'régime', 'juridique', '...']\n",
      "after removing stop words:  ['Un', 'avenant', 'contrat', 'travail', 'peut', 'parfois', 'être', 'refusé', 'salarié', '.', 'Voici', 'droits', 'obligations', 'lorsque', \"l'employeur\", 'demande', 'signer', 'type', 'document', '.', 'Avant', 'toute', 'chose', ';', 'salarié', 'doit', 'savoir', 'régime', 'juridique', '...']\n",
      "before:  ['Meilleure', 'réponse', ':', 'Bonsoir', ';', 'La', 'déclaration', 'mensuelle', 'est', 'obligatoire', 'et', 'doit', 'comportée', 'impérativement', 'tous', 'les', 'revenus', 'que', 'vous', 'avez', 'pu', 'touché', 'pour', 'que', 'justement', 'Pôle', 'Emploi', 'puisse', 'calculé', 'vos', 'droits', '.', 'Pensez', 'à', 'ceci', ':', 'Votre', 'employeur', 'va', 'vous', 'déclaré', '...']\n",
      "after removing stop words:  ['Meilleure', 'réponse', ':', 'Bonsoir', ';', 'La', 'déclaration', 'mensuelle', 'obligatoire', 'doit', 'comportée', 'impérativement', 'tous', 'les', 'revenus', 'pu', 'touché', 'justement', 'Pôle', 'Emploi', 'puisse', 'calculé', 'droits', '.', 'Pensez', 'ceci', ':', 'Votre', 'employeur', 'va', 'déclaré', '...']\n",
      "before:  ['Qui', 'a', 'droit', 'à', \"l'allocation\", 'de', 'rentrée', 'scolaire', '2019', '-', '2020', '(', 'ARS', ')', '.', 'Les', 'conditions', 'à', 'remplir', 'pour', 'percevoir', 'la', 'prime', 'de', 'rentrée', 'scolaire', '.', 'Montant', 'et', 'date', 'de', 'versement', 'de', \"l'ARS\", 'en', '2019', '.', 'Les', 'parents', 'qui', 'remplissent', 'les', 'conditions', 'qui', 'suivent', '...']\n",
      "after removing stop words:  ['Qui', 'a', 'droit', \"l'allocation\", 'rentrée', 'scolaire', '2019', '-', '2020', '(', 'ARS', ')', '.', 'Les', 'conditions', 'remplir', 'percevoir', 'prime', 'rentrée', 'scolaire', '.', 'Montant', 'date', 'versement', \"l'ARS\", '2019', '.', 'Les', 'parents', 'remplissent', 'les', 'conditions', 'suivent', '...']\n",
      "before:  ['Elle', 'est', 'simple', 'à', 'réaliser', 'même', 'pour', 'un', 'débutant', 'en', 'cuisine', 'et', 'prend', 'peu', 'de', 'temps', 'à', 'préparer', '.', 'Pour', 'vous', 'aider', \"j'ai\", 'essayé', \"d'être\", 'le', 'plus', 'clair', 'possible', ';', 'en', 'vous', 'guidant', 'étape', 'par', 'étape', 'pour', 'la', 'préparation', 'de', 'vos', 'crêpes', '.']\n",
      "after removing stop words:  ['Elle', 'simple', 'réaliser', 'débutant', 'cuisine', 'prend', 'peu', 'temps', 'préparer', '.', 'Pour', 'aider', \"j'ai\", 'essayé', \"d'être\", 'plus', 'clair', 'possible', ';', 'guidant', 'étape', 'étape', 'préparation', 'crêpes', '.']\n",
      "before:  ['Le', 'montant', 'maximum', 'de', \"l'allocation\", 'aux', 'adultes', 'handicapés', '(', 'AAH', '2019', ')', 'est', 'de', '860', 'euros', 'depuis', 'le', '1er', 'novembre', '2018', ';', 'date', 'de', 'sa', 'dernière', 'augmentation', '.', 'Ce', 'montant', 'correspond', 'à', \"l'aide\", 'touchée', 'par', 'un', 'bénéficiaire', 'sans', 'ressource', '.', \"L'AAH\", 'est', 'en', '...']\n",
      "after removing stop words:  ['Le', 'montant', 'maximum', \"l'allocation\", 'adultes', 'handicapés', '(', 'AAH', '2019', ')', '860', 'euros', 'depuis', '1er', 'novembre', '2018', ';', 'date', 'dernière', 'augmentation', '.', 'Ce', 'montant', 'correspond', \"l'aide\", 'touchée', 'bénéficiaire', 'sans', 'ressource', '.', \"L'AAH\", '...']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2150a43cb575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1958\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Tokenizing and Removing stopwords\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df1 = df.dropna()\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    sentence = df1.iloc[i,2]\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(\"before: \", tokens)\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    new_tokens = [w for w in tokens if not w in stop_words]\n",
    "    print(\"after removing stop words: \", new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'les': 85, 'meilleures': 93, 'recettes': 133, 'desserts': 49, 'classiques': 34, 'ou': 108, 'originaux': 107, 'réaliser': 142, 'rapidement': 132, 'et': 66, 'facilement': 68, 'la': 83, 'maison': 88, 'rémunération': 144, 'des': 48, 'assistantes': 19, 'maternelles': 90, 'salaire': 148, 'horaire': 73, 'indemnités': 76, 'entretien': 63, 'de': 44, 'nourriture': 101, 'doit': 51, 'respecter': 138, 'certains': 31, 'montants': 98, 'minimum': 95, 'voici': 173, 'un': 169, 'point': 118, 'sur': 158, 'droits': 53, 'assistante': 18, 'maternelle': 89, 'obligations': 104, 'parents': 110, 'en': 61, 'accédez': 7, 'intégralité': 78, 'articles': 17, 'du': 54, 'figaro': 69, 'illimité': 74, 'politique': 119, 'économie': 178, 'culture': 42, 'international': 77, 'retrouvez': 140, 'analyses': 13, 'investigations': 79, 'enquêtes': 62, 'voir': 174, 'le': 84, 'tutoriel': 165, 'vidéo': 172, 'télécharger': 167, 'application': 15, 'free': 70, 'mon': 96, 'compte': 36, 'pour': 121, 'iphone': 80, 'puis': 127, 'saisir': 147, 'son': 156, 'numéro': 103, 'téléphone': 168, 'mot': 99, 'passe': 112, 'écran': 179, 'accueil': 6, 'officiel': 106, 'android': 14, 'avenant': 24, 'au': 20, 'contrat': 38, 'travail': 164, 'peut': 116, 'parfois': 111, 'être': 181, 'refusé': 134, 'par': 109, 'salarié': 149, 'ses': 153, 'lorsque': 86, 'employeur': 60, 'lui': 87, 'demande': 45, 'signer': 154, 'ce': 29, 'type': 166, 'document': 50, 'avant': 23, 'toute': 163, 'chose': 32, 'savoir': 151, 'que': 130, 'régime': 143, 'juridique': 81, 'meilleure': 92, 'réponse': 145, 'bonsoir': 26, 'déclaration': 56, 'mensuelle': 94, 'est': 65, 'obligatoire': 105, 'comportée': 35, 'impérativement': 75, 'tous': 162, 'revenus': 141, 'vous': 177, 'avez': 25, 'pu': 126, 'touché': 160, 'justement': 82, 'pôle': 129, 'emploi': 59, 'puisse': 128, 'calculé': 28, 'vos': 175, 'pensez': 113, 'ceci': 30, 'votre': 176, 'va': 170, 'déclaré': 57, 'qui': 131, 'droit': 52, 'allocation': 12, 'rentrée': 137, 'scolaire': 152, '2019': 2, '2020': 3, 'ars': 16, 'conditions': 37, 'remplir': 135, 'percevoir': 114, 'prime': 123, 'montant': 97, 'date': 43, 'versement': 171, 'remplissent': 136, 'suivent': 157, 'elle': 58, 'simple': 155, 'même': 100, 'débutant': 55, 'cuisine': 41, 'prend': 122, 'peu': 115, 'temps': 159, 'préparer': 125, 'aider': 11, 'ai': 9, 'essayé': 64, 'plus': 117, 'clair': 33, 'possible': 120, 'guidant': 71, 'étape': 180, 'préparation': 124, 'crêpes': 40, 'maximum': 91, 'aux': 22, 'adultes': 8, 'handicapés': 72, 'aah': 5, '860': 4, 'euros': 67, 'depuis': 46, '1er': 0, 'novembre': 102, '2018': 1, 'sa': 146, 'dernière': 47, 'augmentation': 21, 'correspond': 39, 'aide': 10, 'touchée': 161, 'bénéficiaire': 27, 'sans': 150, 'ressource': 139}\n",
      "(9, 182)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 2 1]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "['1er', '2018', '2019', '2020', '860', 'aah', 'accueil', 'accédez', 'adultes', 'ai', 'aide', 'aider', 'allocation', 'analyses', 'android', 'application', 'ars', 'articles', 'assistante', 'assistantes', 'au', 'augmentation', 'aux', 'avant', 'avenant', 'avez', 'bonsoir', 'bénéficiaire', 'calculé', 'ce', 'ceci', 'certains', 'chose', 'clair', 'classiques', 'comportée', 'compte', 'conditions', 'contrat', 'correspond', 'crêpes', 'cuisine', 'culture', 'date', 'de', 'demande', 'depuis', 'dernière', 'des', 'desserts', 'document', 'doit', 'droit', 'droits', 'du', 'débutant', 'déclaration', 'déclaré', 'elle', 'emploi', 'employeur', 'en', 'enquêtes', 'entretien', 'essayé', 'est', 'et', 'euros', 'facilement', 'figaro', 'free', 'guidant', 'handicapés', 'horaire', 'illimité', 'impérativement', 'indemnités', 'international', 'intégralité', 'investigations', 'iphone', 'juridique', 'justement', 'la', 'le', 'les', 'lorsque', 'lui', 'maison', 'maternelle', 'maternelles', 'maximum', 'meilleure', 'meilleures', 'mensuelle', 'minimum', 'mon', 'montant', 'montants', 'mot', 'même', 'nourriture', 'novembre', 'numéro', 'obligations', 'obligatoire', 'officiel', 'originaux', 'ou', 'par', 'parents', 'parfois', 'passe', 'pensez', 'percevoir', 'peu', 'peut', 'plus', 'point', 'politique', 'possible', 'pour', 'prend', 'prime', 'préparation', 'préparer', 'pu', 'puis', 'puisse', 'pôle', 'que', 'qui', 'rapidement', 'recettes', 'refusé', 'remplir', 'remplissent', 'rentrée', 'respecter', 'ressource', 'retrouvez', 'revenus', 'réaliser', 'régime', 'rémunération', 'réponse', 'sa', 'saisir', 'salaire', 'salarié', 'sans', 'savoir', 'scolaire', 'ses', 'signer', 'simple', 'son', 'suivent', 'sur', 'temps', 'touché', 'touchée', 'tous', 'toute', 'travail', 'tutoriel', 'type', 'télécharger', 'téléphone', 'un', 'va', 'versement', 'vidéo', 'voici', 'voir', 'vos', 'votre', 'vous', 'économie', 'écran', 'étape', 'être']\n",
      "182\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 2 1]\n",
      " [1 1 1 ... 0 0 0]]\n",
      "{'les': 85, 'meilleures': 93, 'recettes': 133, 'desserts': 49, 'classiques': 34, 'ou': 108, 'originaux': 107, 'réaliser': 142, 'rapidement': 132, 'et': 66, 'facilement': 68, 'la': 83, 'maison': 88, 'rémunération': 144, 'des': 48, 'assistantes': 19, 'maternelles': 90, 'salaire': 148, 'horaire': 73, 'indemnités': 76, 'entretien': 63, 'de': 44, 'nourriture': 101, 'doit': 51, 'respecter': 138, 'certains': 31, 'montants': 98, 'minimum': 95, 'voici': 173, 'un': 169, 'point': 118, 'sur': 158, 'droits': 53, 'assistante': 18, 'maternelle': 89, 'obligations': 104, 'parents': 110, 'en': 61, 'accédez': 7, 'intégralité': 78, 'articles': 17, 'du': 54, 'figaro': 69, 'illimité': 74, 'politique': 119, 'économie': 178, 'culture': 42, 'international': 77, 'retrouvez': 140, 'analyses': 13, 'investigations': 79, 'enquêtes': 62, 'voir': 174, 'le': 84, 'tutoriel': 165, 'vidéo': 172, 'télécharger': 167, 'application': 15, 'free': 70, 'mon': 96, 'compte': 36, 'pour': 121, 'iphone': 80, 'puis': 127, 'saisir': 147, 'son': 156, 'numéro': 103, 'téléphone': 168, 'mot': 99, 'passe': 112, 'écran': 179, 'accueil': 6, 'officiel': 106, 'android': 14, 'avenant': 24, 'au': 20, 'contrat': 38, 'travail': 164, 'peut': 116, 'parfois': 111, 'être': 181, 'refusé': 134, 'par': 109, 'salarié': 149, 'ses': 153, 'lorsque': 86, 'employeur': 60, 'lui': 87, 'demande': 45, 'signer': 154, 'ce': 29, 'type': 166, 'document': 50, 'avant': 23, 'toute': 163, 'chose': 32, 'savoir': 151, 'que': 130, 'régime': 143, 'juridique': 81, 'meilleure': 92, 'réponse': 145, 'bonsoir': 26, 'déclaration': 56, 'mensuelle': 94, 'est': 65, 'obligatoire': 105, 'comportée': 35, 'impérativement': 75, 'tous': 162, 'revenus': 141, 'vous': 177, 'avez': 25, 'pu': 126, 'touché': 160, 'justement': 82, 'pôle': 129, 'emploi': 59, 'puisse': 128, 'calculé': 28, 'vos': 175, 'pensez': 113, 'ceci': 30, 'votre': 176, 'va': 170, 'déclaré': 57, 'qui': 131, 'droit': 52, 'allocation': 12, 'rentrée': 137, 'scolaire': 152, '2019': 2, '2020': 3, 'ars': 16, 'conditions': 37, 'remplir': 135, 'percevoir': 114, 'prime': 123, 'montant': 97, 'date': 43, 'versement': 171, 'remplissent': 136, 'suivent': 157, 'elle': 58, 'simple': 155, 'même': 100, 'débutant': 55, 'cuisine': 41, 'prend': 122, 'peu': 115, 'temps': 159, 'préparer': 125, 'aider': 11, 'ai': 9, 'essayé': 64, 'plus': 117, 'clair': 33, 'possible': 120, 'guidant': 71, 'étape': 180, 'préparation': 124, 'crêpes': 40, 'maximum': 91, 'aux': 22, 'adultes': 8, 'handicapés': 72, 'aah': 5, '860': 4, 'euros': 67, 'depuis': 46, '1er': 0, 'novembre': 102, '2018': 1, 'sa': 146, 'dernière': 47, 'augmentation': 21, 'correspond': 39, 'aide': 10, 'touchée': 161, 'bénéficiaire': 27, 'sans': 150, 'ressource': 139}\n",
      "[2.60943791 2.60943791 2.2039728  2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.2039728  2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.2039728\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.2039728  1.35667494 2.60943791 2.60943791 2.60943791\n",
      " 2.2039728  2.60943791 2.60943791 1.91629073 2.60943791 1.91629073\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.2039728  1.35667494 2.60943791 2.60943791 2.60943791 1.91629073\n",
      " 1.10536052 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 1.51082562\n",
      " 1.69314718 1.51082562 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.2039728  2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.2039728  2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 1.91629073 2.2039728  2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 1.69314718 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.2039728  2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.2039728  2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.2039728  2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791 2.60943791\n",
      " 2.60943791 1.69314718 2.60943791 2.60943791 2.60943791 2.2039728\n",
      " 2.60943791 2.2039728  2.60943791 2.2039728  2.60943791 2.60943791\n",
      " 2.60943791 2.2039728 ]\n",
      "(1, 182)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.30768122 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30768122 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13033407 0.         0.30768122 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.17814284\n",
      "  0.         0.17814284 0.         0.         0.30768122 0.\n",
      "  0.         0.         0.         0.30768122 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.30768122\n",
      "  0.30768122 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30768122 0.30768122 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.25987246 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Counter Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = list(df[\"description\"].dropna())\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "cv.fit(texts)\n",
    "\n",
    "# encode document\n",
    "vector = cv.transform(texts)\n",
    "\n",
    "print(cv.vocabulary_)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "\n",
    "# both in the same time\n",
    "cv1 = CountVectorizer()\n",
    "cv_fit = cv1.fit_transform(texts)\n",
    "print(cv1.get_feature_names())\n",
    "print(len(cv1.get_feature_names()))\n",
    "print(cv_fit.toarray())\n",
    "\n",
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "vc_tf_idf = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vc_tf_idf.fit(texts)\n",
    "# encode document\n",
    "vector = vc_tf_idf.transform([texts[0]])\n",
    "\n",
    "print(vc_tf_idf.vocabulary_)\n",
    "print(vc_tf_idf.idf_)\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word2vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training w2v: done\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "df2 = pd.read_csv(\"data2.csv\")\n",
    "X_train = df2[\"description\"].dropna()\n",
    "\n",
    "#Initialize model_w2v and build vocabularies\n",
    "#should use the whole dataset, not just training set\n",
    "emb_size = 128\n",
    "model_w2v = Word2Vec(size=emb_size, min_count=5)\n",
    "model_w2v.build_vocab(X_train)\n",
    "model_w2v.train(X_train, total_examples=model_w2v.corpus_count, epochs=2000)\n",
    "model_w2v_path = './model_w2v_urls.bin' # save the model_w2v\n",
    "model_w2v.save(model_w2v_path)\n",
    "print(\"training w2v: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/home/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# reload the trained model_w2v\n",
    "new_model = Word2Vec.load(model_w2v_path)\n",
    "X = new_model[new_model.wv.vocab] # get words\n",
    "\n",
    "#Build word vector for training set by using the average value of all word vectors in the tweet, then scale\n",
    "def build_word2vec_from_text(model_w2v, sentence, emb_size):\n",
    "    emb_vec = np.zeros(emb_size).reshape((1, emb_size))\n",
    "    count = 0.\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            emb_vec += model_w2v[word].reshape((1, emb_size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        emb_vec /= count\n",
    "    return emb_vec\n",
    "\n",
    "X_train = np.concatenate([build_word2vec_from_text(model_w2v, d, emb_size) for d in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13549828,  0.13148418, -0.09423224, ..., -0.03559573,\n",
       "         0.28165428,  0.14442378],\n",
       "       [-0.14830357,  0.11263042, -0.14316868, ..., -0.03900621,\n",
       "         0.30740614,  0.12885911],\n",
       "       [-0.16314488,  0.16973354, -0.10512566, ..., -0.05694924,\n",
       "         0.31077937,  0.18561209],\n",
       "       ...,\n",
       "       [-0.13354129,  0.11853259, -0.07228647, ..., -0.0684135 ,\n",
       "         0.25152667,  0.14634185],\n",
       "       [-0.17379498,  0.12090287,  0.02751243, ..., -0.20319484,\n",
       "         0.28882187,  0.20464262],\n",
       "       [-0.14928744,  0.13425103, -0.10519732, ..., -0.06861092,\n",
       "         0.23458761,  0.17386866]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorizing with TF-IDF and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df2 = pd.read_csv(\"data2.csv\")\n",
    "texts = list(df2[\"description\"])\n",
    "NBR_CLUSTER_TEST = 50\n",
    "\n",
    "vc_tf_idf = TfidfVectorizer(max_df=0.95, min_df=2,max_features=128)\n",
    "vc_tf_idf.fit(texts)\n",
    "vect_array = []\n",
    "flat_array = []\n",
    "wcss = []\n",
    "for k in range(0,len(texts)):\n",
    "    vector = vc_tf_idf.transform([texts[k]])\n",
    "    vect_array.append(vector.toarray())\n",
    "flat_array = [item for sublist in vect_array for item in sublist]\n",
    "\n",
    "for i in range(1, NBR_CLUSTER_TEST):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(flat_array)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, NBR_CLUSTER_TEST), wcss)\n",
    "plt.title('The elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
